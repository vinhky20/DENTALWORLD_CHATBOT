{
  "ask_price": {
    "precision": 0.9,
    "recall": 1.0,
    "f1-score": 0.9473684210526316,
    "support": 45,
    "confused_with": {}
  },
  "get_phone": {
    "precision": 0.9629629629629629,
    "recall": 1.0,
    "f1-score": 0.9811320754716981,
    "support": 26,
    "confused_with": {}
  },
  "thank": {
    "precision": 1.0,
    "recall": 0.8571428571428571,
    "f1-score": 0.923076923076923,
    "support": 7,
    "confused_with": {
      "greet": 1
    }
  },
  "get_email": {
    "precision": 1.0,
    "recall": 0.9523809523809523,
    "f1-score": 0.975609756097561,
    "support": 21,
    "confused_with": {
      "get_phone": 1
    }
  },
  "ask_worktime": {
    "precision": 0.6666666666666666,
    "recall": 0.8,
    "f1-score": 0.7272727272727272,
    "support": 15,
    "confused_with": {
      "ask_address": 1,
      "ask_list_clinic": 1
    }
  },
  "ask_timetodo": {
    "precision": 0.9782608695652174,
    "recall": 1.0,
    "f1-score": 0.989010989010989,
    "support": 45,
    "confused_with": {}
  },
  "goodbye": {
    "precision": 1.0,
    "recall": 0.9444444444444444,
    "f1-score": 0.9714285714285714,
    "support": 18,
    "confused_with": {
      "get_gender": 1
    }
  },
  "greet": {
    "precision": 0.8648648648648649,
    "recall": 0.8888888888888888,
    "f1-score": 0.8767123287671232,
    "support": 36,
    "confused_with": {
      "get_name": 2,
      "ask_service": 1
    }
  },
  "ask": {
    "precision": 1.0,
    "recall": 0.35714285714285715,
    "f1-score": 0.5263157894736842,
    "support": 14,
    "confused_with": {
      "ask_price": 4,
      "ask_service": 3
    }
  },
  "request_schedule": {
    "precision": 0.6875,
    "recall": 0.7857142857142857,
    "f1-score": 0.7333333333333334,
    "support": 28,
    "confused_with": {
      "inform": 5,
      "ask_worktime": 1
    }
  },
  "deny": {
    "precision": 0.0,
    "recall": 0.0,
    "f1-score": 0.0,
    "support": 4,
    "confused_with": {
      "get_note": 3,
      "get_gender": 1
    }
  },
  "ask_service": {
    "precision": 0.8780487804878049,
    "recall": 1.0,
    "f1-score": 0.9350649350649352,
    "support": 36,
    "confused_with": {}
  },
  "get_service": {
    "precision": 0.8461538461538461,
    "recall": 1.0,
    "f1-score": 0.9166666666666666,
    "support": 11,
    "confused_with": {}
  },
  "get_name": {
    "precision": 0.7222222222222222,
    "recall": 0.5,
    "f1-score": 0.5909090909090908,
    "support": 26,
    "confused_with": {
      "inform": 5,
      "affirm": 3
    }
  },
  "ask_codauko": {
    "precision": 0.9285714285714286,
    "recall": 0.9285714285714286,
    "f1-score": 0.9285714285714286,
    "support": 28,
    "confused_with": {
      "greet": 1,
      "get_service": 1
    }
  },
  "day_booking": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 34,
    "confused_with": {}
  },
  "ask_list_clinic": {
    "precision": 0.6666666666666666,
    "recall": 0.5714285714285714,
    "f1-score": 0.6153846153846153,
    "support": 7,
    "confused_with": {
      "ask_worktime": 3
    }
  },
  "inform": {
    "precision": 0.45454545454545453,
    "recall": 0.5555555555555556,
    "f1-score": 0.5,
    "support": 18,
    "confused_with": {
      "request_schedule": 4,
      "get_name": 2
    }
  },
  "get_note": {
    "precision": 0.36363636363636365,
    "recall": 0.5714285714285714,
    "f1-score": 0.4444444444444444,
    "support": 7,
    "confused_with": {
      "inform": 2,
      "ask_codauko": 1
    }
  },
  "get_gender": {
    "precision": 0.0,
    "recall": 0.0,
    "f1-score": 0.0,
    "support": 9,
    "confused_with": {
      "affirm": 2,
      "greet": 2,
      "get_note": 2
    }
  },
  "ask_address": {
    "precision": 0.875,
    "recall": 1.0,
    "f1-score": 0.9333333333333333,
    "support": 7,
    "confused_with": {}
  },
  "affirm": {
    "precision": 0.6428571428571429,
    "recall": 0.6923076923076923,
    "f1-score": 0.6666666666666666,
    "support": 13,
    "confused_with": {
      "ask_worktime": 1,
      "greet": 1
    }
  },
  "accuracy": 0.843956043956044,
  "macro avg": {
    "precision": 0.7471798758727565,
    "recall": 0.7456820956820956,
    "f1-score": 0.7355591861830192,
    "support": 455
  },
  "weighted avg": {
    "precision": 0.8340993840367479,
    "recall": 0.843956043956044,
    "f1-score": 0.8311695932949706,
    "support": 455
  }
}